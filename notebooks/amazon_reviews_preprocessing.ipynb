{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnnLnFnLlDEU"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "## Table of Contents\n",
        "\n",
        "</div>\n",
        "\n",
        "[1. Introduction](#Intro) <br>\n",
        "[2. Importing Libraries](#libs) <br>\n",
        "[3. Examining Input File](#examine) <br>\n",
        "[4. Loading and Parsing Files](#load) <br>\n",
        "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
        "$\\;\\;\\;\\;$[4.2. Genegrate numerical representation](#whetev1) <br>\n",
        "[5. Writing Output Files](#write) <br>\n",
        "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
        "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
        "[6. Summary](#summary) <br>\n",
        "[7. References](#Ref) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8mo6PPRlDEU"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewZrff73lDEV"
      },
      "source": [
        "This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing:\n",
        "\n",
        "  - category: Product category\n",
        "  -reviewer_id: Anonymous ID of the reviewe\n",
        "  -rating: Numerical rating\n",
        "  -review_title: Short summary of the review\n",
        "  -review_text: Full content of the user's feedback\n",
        "  -attached_images: Whether an image is included\n",
        "  - product_id & parent_product_id: Unique identifiers for product and parent group\n",
        "  -review_timestamp: Date and time when the review was posted\n",
        "  -is_verified_purchase: Indicates if the reviewer actually purchased the item\n",
        "  -helpful_votes: Number of users who found the review helpful\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSr_kwKclDEV"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acwZw2NklDEW"
      },
      "source": [
        "In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:\n",
        "\n",
        "* **os:** to interact with the operating system, e.g. navigate through folders to read files\n",
        "* **re:** to define and use regular expressions\n",
        "* **pandas:** to work with dataframes\n",
        "* **multiprocessing:** to perform processes on multi cores for fast performance\n",
        "* **collections:** to use defaultdict and Counter for counting and grouping operations\n",
        "* **math:** to apply logarithmic functions such as log2 during PMI calculations\n",
        "* **nltk.stem:** to apply Porter stemming and reduce tokens to their root form\n",
        "* **nltk.collocations:** to find frequently co-occurring word pairs using BigramCollocationFinder and BigramAssocMeasures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qgmGWs8HlDEW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "from math import log2\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwNp0KnWlDEX"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "SA7fSJiRlDEY"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPCuEl8smTHW",
        "outputId": "9d0150ad-bdf7-4e3e-9373-7cdf75734c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CJDLDI6lDEY"
      },
      "source": [
        "Let's examine what is the content of the file. For this purpose, print the contents of the first two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyLbqkRxmCEZ",
        "outputId": "cff85e91-e4bc-4a58-8767-90f9d13c59b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parent Product ID: 0176496920\n",
            "Reviews Example:\n",
            "category: industrial_and_scientific\n",
            "reviewer_id: aheutzza5vraknyxplzvchlot3na\n",
            "rating: 5.0\n",
            "review_title: very good\n",
            "review_text: just what i needed for pharm school\n",
            "attached_images: none\n",
            "product_id: 0176496920\n",
            "review_timestamp: 2017-08-15 01:45:00\n",
            "is_verified_purchase: True\n",
            "helpful_votes: 0\n",
            "------------------------------\n",
            "category: industrial_and_scientific\n",
            "reviewer_id: ahy7vyqny6k3zoy7ee3r3efpngoa\n",
            "rating: 5.0\n",
            "review_title: great device\n",
            "review_text: this device was hard to find for my daughter's ot school class from the description provided by the teacher. fortunately, the key words were adequate to find the correct device, and it works as needed.\n",
            "attached_images: none\n",
            "product_id: 1934931403\n",
            "review_timestamp: 2012-11-06 07:09:05\n",
            "is_verified_purchase: True\n",
            "helpful_votes: 1\n",
            "------------------------------\n",
            "Parent Product ID: 0201336421\n",
            "Reviews Example:\n",
            "category: industrial_and_scientific\n",
            "reviewer_id: agkh3hpiglkor3wxzeduz65jmnha\n",
            "rating: 1.0\n",
            "review_title: not working\n",
            "review_text: its not working and make damage to my computer\n",
            "attached_images: none\n",
            "product_id: 0201336421\n",
            "review_timestamp: 2021-02-14 01:17:15\n",
            "is_verified_purchase: True\n",
            "helpful_votes: 0\n",
            "------------------------------\n",
            "category: industrial_and_scientific\n",
            "reviewer_id: afepym5nkvy65ya7xe526kkjnuta\n",
            "rating: 5.0\n",
            "review_title: its worth it for mobile mechanis\n",
            "review_text: nice thing with this is i can hook it to my tablet or my phone for on the road diag when i cant get it to the shop and need a repair in the field\n",
            "attached_images: none\n",
            "product_id: 0201336421\n",
            "review_timestamp: 2022-10-24 03:27:10\n",
            "is_verified_purchase: True\n",
            "helpful_votes: 0\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/drive/MyDrive/task1_group_168.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# View the contents of the first two parent_product_id in the data:\n",
        "for i, (pid, content) in enumerate(data.items()):\n",
        "    print(f\"Parent Product ID: {pid}\")\n",
        "    print(\"Reviews Example:\")\n",
        "    for review in content.get(\"reviews\", [])[:2]:\n",
        "        for k, v in review.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "        print(\"-\" * 30)\n",
        "    if i >= 1:  # Only 2\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpDVyW4YlDEZ"
      },
      "source": [
        "It is noticed that the file contains nested JSON structures, where each `parent_product_id` maps to multiple review records.\n",
        "\n",
        "Each review entry includes key fields such as `rating`, `review_text`, `review_title`, `review_timestamp`, `is_verified_purchase`, and `helpful_votes`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7c1-c3olDEa"
      },
      "source": [
        "Having parsed the pdf file, the following observations can be made: ....\n",
        "Having parsed the file, the following observations can be made:\n",
        "\n",
        "- Most reviews have 5-star ratings, indicating a general trend of user satisfaction.\n",
        "- Some products contain both positive and negative feedback, as seen in review text variations.\n",
        "- The `is_verified_purchase` field can be useful for analyzing review credibility.\n",
        "- The `review_timestamp` allows for potential time-based trend analysis.\n",
        "- The field `attached_images` is often marked as \"none\", suggesting image-based analysis may be limited.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ENnHWjoXlDEc"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9esGMx8lDEc"
      },
      "source": [
        "In this section, we perform text extraction from the original dataset.\n",
        "We load the task1_group168.json file, which contains the full review dataset.\n",
        "\n",
        "The goal is to extract parent_product_id and the corresponding review_text for each entry.\n",
        "\n",
        "We filter out reviews where review_text is \"none\" to retain only valid records.\n",
        "This step ensures that the reviews we process later are meaningful and not missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsnRR2c4lDEc",
        "outputId": "903bbc77-96ff-4b44-8a7f-da071f8bc4b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of original records: 278238\n",
            "  parent_product_id                                        review_text\n",
            "0        0176496920                just what i needed for pharm school\n",
            "1        0176496920  this device was hard to find for my daughter's...\n",
            "2        0176496920                                       works great!\n",
            "3        0201336421     its not working and make damage to my computer\n",
            "4        0201336421  nice thing with this is i can hook it to my ta...\n",
            "Total number of valid review_text: 277735\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/drive/MyDrive/task1_group_168.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "records = []\n",
        "for parent_id, group in data.items():\n",
        "    for review in group.get(\"reviews\", []):\n",
        "        records.append({\n",
        "            \"parent_product_id\": parent_id,\n",
        "            \"review_text\": review.get(\"review_text\", \"none\")\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "print(\"Total number of original records:\", df.shape[0])\n",
        "print(df.head())\n",
        "\n",
        "df_valid = df[df[\"review_text\"].str.lower() != \"none\"]\n",
        "print(\"Total number of valid review_text:\", df_valid.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uh9oUXAlDEd"
      },
      "source": [
        "Let's examine the dictionary generated. For counting the total number of reviews extracted, we identify products with a sufficient number of reviews.\n",
        "\n",
        "We group the valid review data by parent_product_id and retain only those products that have at least 50 reviews, as required by the task guideline.\n",
        "\n",
        "This ensures that our analysis is based on robust and representative review samples.\n",
        "\n",
        "The filtered dataset is stored in df_task2, which will be used for further token processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUZuFeuQlDEd",
        "outputId": "f1741e69-5ef5-4f89-d677-591f325825a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parent_product_ids with ≥50 reviews: 434\n",
            "The number of records after final filtering: (57326, 2)\n",
            "     parent_product_id                                        review_text\n",
            "1572        b0002bbx3u                                      great product\n",
            "1573        b0002bbx3u  description mentions electronics, but this isn...\n",
            "1574        b0002bbx3u  my toilet tank was cracked and seeping, not dr...\n",
            "1575        b0002bbx3u                                            perfect\n",
            "1576        b0002bbx3u    i used it to seal the draining pipe, works good\n"
          ]
        }
      ],
      "source": [
        "valid_parents = df_valid.groupby(\"parent_product_id\") \\\n",
        "                        .filter(lambda x: len(x) >= 50)[\"parent_product_id\"] \\\n",
        "                        .unique()\n",
        "\n",
        "print(\"Number of parent_product_ids with ≥50 reviews:\", len(valid_parents))\n",
        "\n",
        "df_task2 = df_valid[df_valid[\"parent_product_id\"].isin(valid_parents)].copy()\n",
        "\n",
        "print(\"The number of records after final filtering:\", df_task2.shape)\n",
        "print(df_task2[[\"parent_product_id\", \"review_text\"]].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VIfQCD1VlDEe"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.1 Tokenization <a class=\"anchor\" name=\"tokenize\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gchByyjolDEf"
      },
      "source": [
        "Tokenization is a principal step in text processing and producing unigrams. In this section, we clean and tokenize the review texts.\n",
        "\n",
        "We begin by loading a list of independent stopwords from a file, then initialize a regex pattern to extract words.\n",
        "\n",
        "Each review text is converted to lowercase, tokenized using the regex, filtered to remove stopwords and short words (less than 3 characters), and stemmed using PorterStemmer.\n",
        "\n",
        "The final tokens are stored in a dictionary where each key is a parent_product_id and the value is a list of cleaned tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p8zT4N0RlDEf"
      },
      "outputs": [],
      "source": [
        "# Load stopwords_en.txt\n",
        "with open(\"/content/drive/MyDrive/stopwords_en.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    stopwords_indep = set(f.read().splitlines())\n",
        "\n",
        "# Initialization\n",
        "stemmer = PorterStemmer()\n",
        "token_pattern = re.compile(r\"[a-zA-Z]+\")\n",
        "\n",
        "# Build: Each parent_product_id → token list\n",
        "product_tokens = defaultdict(list)\n",
        "\n",
        "for _, row in df_task2.iterrows():\n",
        "    parent_id = row[\"parent_product_id\"]\n",
        "    text = row[\"review_text\"].lower()\n",
        "\n",
        "    tokens = token_pattern.findall(text)  # Regular word segmentation\n",
        "    tokens = [t for t in tokens if t not in stopwords_indep]  # Remove short words + stemming\n",
        "    tokens = [stemmer.stem(t) for t in tokens if len(t) >= 3]  # Remove independent stopwords\n",
        "\n",
        "    product_tokens[parent_id].extend(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZos1q6lDEf"
      },
      "source": [
        "The above operation results in a dictionary with PID representing keys and a single string for all reviews of the day concatenated to each other.\n",
        "\n",
        "We count how frequently each token appears across different products.\n",
        "We determine how many unique products each token appears in.\n",
        "\n",
        "Tokens that appear in too many products (≥95%) or too few (≤5%) are filtered out, as these are considered either too generic or too rare.\n",
        "\n",
        "This step helps us retain contextually meaningful and discriminative tokens, which will improve the quality of downstream analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPBNTTq6lDEg",
        "outputId": "7eada059-523d-4712-fc2b-9e81a595cc25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of remaining tokens after filtering: 2046\n"
          ]
        }
      ],
      "source": [
        "# Count how many parent_product_ids each token appears in\n",
        "token_product_occurrence = defaultdict(set)\n",
        "\n",
        "for parent_id, tokens in product_tokens.items():\n",
        "    for token in set(tokens):  # # Use set to remove duplicates to prevent the same product from being counted twice\n",
        "        token_product_occurrence[token].add(parent_id)\n",
        "\n",
        "# Convert to token -> Number of products that appear\n",
        "token_df = pd.DataFrame([\n",
        "    {\"token\": token, \"product_count\": len(pid_set)}\n",
        "    for token, pid_set in token_product_occurrence.items()\n",
        "])\n",
        "\n",
        "# Get the total number of products\n",
        "total_products = len(product_tokens)\n",
        "\n",
        "# Setting the Threshold\n",
        "high_thresh = 0.95 * total_products\n",
        "low_thresh = 0.05 * total_products\n",
        "\n",
        "# Filter out:\n",
        "# Appearing in 95%+ of products (context-dependent stopwords)\n",
        "# Appearing in 5%- of products (rare tokens)\n",
        "filtered_tokens = token_df[\n",
        "    (token_df[\"product_count\"] < high_thresh) &\n",
        "    (token_df[\"product_count\"] >= low_thresh)\n",
        "][\"token\"].tolist()\n",
        "\n",
        "print(f\"Number of remaining tokens after filtering: {len(filtered_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVqFfwwMlDEg"
      },
      "source": [
        "At this stage, all reviews for each PID are tokenized and are stored as a value in the new dictionary (separetely for each day).\n",
        "\n",
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ve6IZ2I-lDEg"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.2 Generate numerical representation<a class=\"anchor\" name=\"bigrams\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erGhUY2UlDEg"
      },
      "source": [
        "One of the tasks is to generate the numerical representation for all tokens in abstract.\n",
        "\n",
        "In this section, we generate bigram tokens and calculate their significance using the Pointwise Mutual Information (PMI) metric.\n",
        "\n",
        "We apply frequency filtering to remove rare bigrams and extract the top 200 most significant bigrams.\n",
        "\n",
        "These are then formatted and added to the vocabulary for later vector representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgFFtm6qlDEg",
        "outputId": "19bdd9fb-13e7-49f0-ecda-e1ab72f8ae61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 PMI bigrams (NLTK):\n",
            "1. harbor_freight\n",
            "2. fcil_usar\n",
            "3. golden_retriev\n",
            "4. game_changer\n",
            "5. allerg_reaction\n"
          ]
        }
      ],
      "source": [
        "# Prepare the data\n",
        "tokenized_reviews = []\n",
        "\n",
        "for _, row in df_task2.iterrows():\n",
        "    text = row[\"review_text\"].lower()\n",
        "    tokens = token_pattern.findall(text)\n",
        "    tokens = [t for t in tokens if t not in stopwords_indep]\n",
        "    tokens = [stemmer.stem(t) for t in tokens if len(t) >= 3]\n",
        "    tokens = [t for t in tokens if t in filtered_tokens]\n",
        "    if tokens:\n",
        "        tokenized_reviews.append(tokens)\n",
        "\n",
        "# Constructing PMI model using BigramCollocationFinder\n",
        "finder = BigramCollocationFinder.from_documents(tokenized_reviews)\n",
        "\n",
        "# Filter out low frequency bigram\n",
        "finder.apply_freq_filter(2)\n",
        "\n",
        "# Extract the first 200 PMI bigrams\n",
        "pmi = BigramAssocMeasures()\n",
        "top_bigrams = finder.nbest(pmi.pmi, 200)\n",
        "\n",
        "# Format Conversion\n",
        "top_bigrams_str = [f\"{w1}_{w2}\" for w1, w2 in top_bigrams]\n",
        "\n",
        "print(\"Top 5 PMI bigrams (NLTK):\")\n",
        "for i, b in enumerate(top_bigrams_str[:5], 1):\n",
        "    print(f\"{i}. {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO1PJO-dlDEh"
      },
      "source": [
        "At this stage, we have a dictionary of tokenized words, whose keys are indicative of the most informative unigrams and bigrams appearing across product reviews. These tokens—refined through stopword filtering, stemming, and PMI-based selection—form the foundation of our final vocabulary. This vocabulary will later be used to construct the sparse matrix representation for each product.\n",
        "\n",
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmaGJYIJlDEl"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjMBqRetlDEl"
      },
      "source": [
        "Files need to be generated:\n",
        "* Vocabulary list\n",
        "* Sparse matrix (count_vectors)\n",
        "\n",
        "This is performed in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc6tQ4ljlDEm"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 5.1 Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlbpGYilDEm"
      },
      "source": [
        "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file.\n",
        "\n",
        "For this purpose, we merge the filtered unigrams and top bigrams, sort the combined list alphabetically, and reassign each token a new index.\n",
        "\n",
        "The final vocabulary is saved into vocab.txt, which is referenced when creating the sparse representation in the next step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6OUXHlxlDEm",
        "outputId": "b5b2fcd2-955a-4d4e-c799-2bcd99acdeae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The alphabetically sorted vocabulary has been saved to vocab.txt, total 2246 items.\n"
          ]
        }
      ],
      "source": [
        "# Merge and sort alphabetically\n",
        "vocab_all = sorted(filtered_tokens + top_bigrams_str)\n",
        "\n",
        "# Reassign indices to each token\n",
        "with open(\"group_168_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, token in enumerate(vocab_all):\n",
        "        f.write(f\"{token}:{idx}\\n\")\n",
        "\n",
        "print(f\"The alphabetically sorted vocabulary has been saved to vocab.txt, total {len(vocab_all)} items.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CKCg8XFKaqvf",
        "outputId": "eba357e8-4293-4b6d-87d1-5e7158ae39ff"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b18b31a5-0e11-4d27-8d88-27591f978b63\", \"group_168_vocab.txt\", 25534)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"group_168_vocab.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkGH81YFlDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 5.2 Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtxqUAwmlDEn"
      },
      "source": [
        "For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper.\n",
        "\n",
        "In this section, we build the countvec.txt file by constructing a sparse matrix.\n",
        "\n",
        "Each product's reviews are combined, tokenized, and processed into unigrams and bigrams.\n",
        "\n",
        "The frequency of each token is recorded using the index mapping from the vocabulary file, and the final matrix is saved in the format required for downstream machine learning or text analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__n1fdIqlDEn",
        "outputId": "0a9501c8-f157-4bc8-c28a-b973eaa2e750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparse representation saved to countvec.txt — 434 lines.\n"
          ]
        }
      ],
      "source": [
        "# Create token → index mapping (based on vocab.txt)\n",
        "token_to_index = {token: idx for idx, token in enumerate(vocab_all)}\n",
        "\n",
        "# Group reviews by product and count token frequencies\n",
        "product_vectors = {}\n",
        "\n",
        "for parent_id, group in df_task2.groupby(\"parent_product_id\"):\n",
        "    # Concatenate all review texts for this product\n",
        "    all_text = \" \".join(group[\"review_text\"].str.lower().tolist())\n",
        "\n",
        "    # Tokenize and stem\n",
        "    tokens = token_pattern.findall(all_text)\n",
        "    tokens = [t for t in tokens if t not in stopwords_indep]\n",
        "    tokens = [stemmer.stem(t) for t in tokens if len(t) >= 3]\n",
        "\n",
        "    # Also generate bigrams\n",
        "    unigrams = [t for t in tokens if t in token_to_index]\n",
        "    bigrams = [f\"{w1}_{w2}\" for w1, w2 in zip(tokens, tokens[1:]) if f\"{w1}_{w2}\" in token_to_index]\n",
        "\n",
        "    # Combine all tokens and count\n",
        "    all_used_tokens = unigrams + bigrams\n",
        "    freq = Counter(all_used_tokens)\n",
        "\n",
        "    # Convert to sparse string: \"index:count\"\n",
        "    sparse_items = [f\"{token_to_index[token]}:{count}\" for token, count in sorted(freq.items(), key=lambda x: token_to_index[x[0]])]\n",
        "    product_vectors[parent_id] = sparse_items\n",
        "\n",
        "# Write to countvec.txt\n",
        "with open(\"group_168_countvec.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for parent_id, sparse_items in product_vectors.items():\n",
        "        line = f\"{parent_id},\" + \",\".join(sparse_items)\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(f\"Sparse representation saved to countvec.txt — {len(product_vectors)} lines.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1rNmxaxqavsU",
        "outputId": "f651737a-7886-426c-e242-de5170b123e4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_fa10b16d-5240-407d-8e10-03be4945e921\", \"group_168_countvec.txt\", 1116562)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"group_168_countvec.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUFQU-QXlDEn"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWjri6x_lDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAprlSblDEn"
      },
      "source": [
        "Through a structured process of review text extraction, preprocessing, and frequency analysis, two key output files were generated:\n",
        "\n",
        "**vocab.txt** (Total: 2,246 items)\n",
        "\n",
        "  - This file contains an alphabetically sorted list of both unigrams and bigrams extracted from helpful product reviews. Each token is paired with a unique index, which serves as a reference for building the sparse matrix.\n",
        "\n",
        "  - Contents:\n",
        "\n",
        "    - Unigrams and PMI-selected bigrams\n",
        "\n",
        "    - Sorted alphabetically\n",
        "\n",
        "    - Indexed for easy lookup\n",
        "\n",
        "**countvec.txt** (Total: 434 lines)\n",
        "\n",
        "  - This file represents the sparse matrix format, where each line corresponds to a unique parent_product_id and lists the frequency of associated tokens in the format token_index:count.\n",
        "\n",
        "  - Contents:\n",
        "\n",
        "    - Token frequencies by product\n",
        "\n",
        "    - Compact representation for downstream modeling or clustering\n",
        "\n",
        "These two files together enable numerical representation of textual data, which can be used for various tasks such as similarity detection, product clustering, or further machine learning applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFXYKxO8lDEn"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HppxDtWNlDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 7. References <a class=\"anchor\" name=\"Ref\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCkWr-M1lDEo"
      },
      "source": [
        "[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 13/08/2022.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp9O-a1UlDEo"
      },
      "source": [
        "## --------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
